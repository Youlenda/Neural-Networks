{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2-q4-2layer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZXEmRvhkdA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('house_data.csv')\n",
        "dataset = df.values\n",
        "\n",
        "# df\n",
        "# dataset.shape, dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEE1aQONkmFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = dataset[:, 0:13]\n",
        "y = dataset[:, 13]\n",
        "\n",
        "# x.shape, y.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq8d2f-VknK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_scale = min_max_scaler.fit_transform(x)\n",
        "TransformY = preprocessing.MinMaxScaler()\n",
        "y_scale = TransformY.fit_transform(y.reshape(y.shape[0],1))\n",
        "\n",
        "#x_scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQHIMaAGkpan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_scale, y_scale, test_size=0.2)\n",
        "\n",
        "# x_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khn0ZDpikrUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#8,6,1\n",
        "model = Sequential([\n",
        "    Dense(10, activation='relu', input_shape=(13,)),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, kernel_initializer='normal')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTShNw3vkueY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2ieg_oqlMF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import timeit\n",
        "start = timeit.default_timer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGK5VH17lPjC",
        "colab_type": "code",
        "outputId": "fab9c5b0-5e2d-43d6-c56c-ccb9bddcb1d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trained_model = model.fit(x_train, y_train, batch_size=16, epochs=100, validation_split=0.2)\n",
        "history = trained_model.history"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 323 samples, validate on 81 samples\n",
            "Epoch 1/100\n",
            "323/323 [==============================] - 1s 4ms/step - loss: 0.1764 - val_loss: 0.1689\n",
            "Epoch 2/100\n",
            "323/323 [==============================] - 0s 361us/step - loss: 0.1394 - val_loss: 0.1265\n",
            "Epoch 3/100\n",
            "323/323 [==============================] - 0s 328us/step - loss: 0.1005 - val_loss: 0.0909\n",
            "Epoch 4/100\n",
            "323/323 [==============================] - 0s 332us/step - loss: 0.0714 - val_loss: 0.0665\n",
            "Epoch 5/100\n",
            "323/323 [==============================] - 0s 342us/step - loss: 0.0558 - val_loss: 0.0547\n",
            "Epoch 6/100\n",
            "323/323 [==============================] - 0s 323us/step - loss: 0.0479 - val_loss: 0.0470\n",
            "Epoch 7/100\n",
            "323/323 [==============================] - 0s 330us/step - loss: 0.0413 - val_loss: 0.0407\n",
            "Epoch 8/100\n",
            "323/323 [==============================] - 0s 322us/step - loss: 0.0359 - val_loss: 0.0349\n",
            "Epoch 9/100\n",
            "323/323 [==============================] - 0s 309us/step - loss: 0.0316 - val_loss: 0.0299\n",
            "Epoch 10/100\n",
            "323/323 [==============================] - 0s 319us/step - loss: 0.0280 - val_loss: 0.0264\n",
            "Epoch 11/100\n",
            "323/323 [==============================] - 0s 311us/step - loss: 0.0258 - val_loss: 0.0244\n",
            "Epoch 12/100\n",
            "323/323 [==============================] - 0s 326us/step - loss: 0.0244 - val_loss: 0.0231\n",
            "Epoch 13/100\n",
            "323/323 [==============================] - 0s 327us/step - loss: 0.0233 - val_loss: 0.0216\n",
            "Epoch 14/100\n",
            "323/323 [==============================] - 0s 354us/step - loss: 0.0222 - val_loss: 0.0203\n",
            "Epoch 15/100\n",
            "323/323 [==============================] - 0s 319us/step - loss: 0.0210 - val_loss: 0.0186\n",
            "Epoch 16/100\n",
            "323/323 [==============================] - 0s 316us/step - loss: 0.0195 - val_loss: 0.0168\n",
            "Epoch 17/100\n",
            "323/323 [==============================] - 0s 291us/step - loss: 0.0181 - val_loss: 0.0152\n",
            "Epoch 18/100\n",
            "323/323 [==============================] - 0s 347us/step - loss: 0.0166 - val_loss: 0.0135\n",
            "Epoch 19/100\n",
            "323/323 [==============================] - 0s 328us/step - loss: 0.0151 - val_loss: 0.0126\n",
            "Epoch 20/100\n",
            "323/323 [==============================] - 0s 290us/step - loss: 0.0140 - val_loss: 0.0118\n",
            "Epoch 21/100\n",
            "323/323 [==============================] - 0s 331us/step - loss: 0.0130 - val_loss: 0.0113\n",
            "Epoch 22/100\n",
            "323/323 [==============================] - 0s 297us/step - loss: 0.0126 - val_loss: 0.0112\n",
            "Epoch 23/100\n",
            "323/323 [==============================] - 0s 321us/step - loss: 0.0121 - val_loss: 0.0107\n",
            "Epoch 24/100\n",
            "323/323 [==============================] - 0s 322us/step - loss: 0.0117 - val_loss: 0.0105\n",
            "Epoch 25/100\n",
            "323/323 [==============================] - 0s 342us/step - loss: 0.0114 - val_loss: 0.0104\n",
            "Epoch 26/100\n",
            "323/323 [==============================] - 0s 323us/step - loss: 0.0112 - val_loss: 0.0101\n",
            "Epoch 27/100\n",
            "323/323 [==============================] - 0s 304us/step - loss: 0.0111 - val_loss: 0.0100\n",
            "Epoch 28/100\n",
            "323/323 [==============================] - 0s 314us/step - loss: 0.0107 - val_loss: 0.0100\n",
            "Epoch 29/100\n",
            "323/323 [==============================] - 0s 323us/step - loss: 0.0107 - val_loss: 0.0098\n",
            "Epoch 30/100\n",
            "323/323 [==============================] - 0s 317us/step - loss: 0.0105 - val_loss: 0.0097\n",
            "Epoch 31/100\n",
            "323/323 [==============================] - 0s 308us/step - loss: 0.0104 - val_loss: 0.0097\n",
            "Epoch 32/100\n",
            "323/323 [==============================] - 0s 314us/step - loss: 0.0101 - val_loss: 0.0099\n",
            "Epoch 33/100\n",
            "323/323 [==============================] - 0s 321us/step - loss: 0.0101 - val_loss: 0.0097\n",
            "Epoch 34/100\n",
            "323/323 [==============================] - 0s 348us/step - loss: 0.0101 - val_loss: 0.0101\n",
            "Epoch 35/100\n",
            "323/323 [==============================] - 0s 313us/step - loss: 0.0099 - val_loss: 0.0097\n",
            "Epoch 36/100\n",
            "323/323 [==============================] - 0s 314us/step - loss: 0.0099 - val_loss: 0.0098\n",
            "Epoch 37/100\n",
            "323/323 [==============================] - 0s 298us/step - loss: 0.0096 - val_loss: 0.0095\n",
            "Epoch 38/100\n",
            "323/323 [==============================] - 0s 320us/step - loss: 0.0095 - val_loss: 0.0095\n",
            "Epoch 39/100\n",
            "323/323 [==============================] - 0s 347us/step - loss: 0.0095 - val_loss: 0.0094\n",
            "Epoch 40/100\n",
            "323/323 [==============================] - 0s 322us/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 41/100\n",
            "323/323 [==============================] - 0s 300us/step - loss: 0.0095 - val_loss: 0.0095\n",
            "Epoch 42/100\n",
            "323/323 [==============================] - 0s 295us/step - loss: 0.0095 - val_loss: 0.0095\n",
            "Epoch 43/100\n",
            "323/323 [==============================] - 0s 337us/step - loss: 0.0093 - val_loss: 0.0095\n",
            "Epoch 44/100\n",
            "323/323 [==============================] - 0s 327us/step - loss: 0.0097 - val_loss: 0.0096\n",
            "Epoch 45/100\n",
            "323/323 [==============================] - 0s 304us/step - loss: 0.0096 - val_loss: 0.0094\n",
            "Epoch 46/100\n",
            "323/323 [==============================] - 0s 319us/step - loss: 0.0095 - val_loss: 0.0092\n",
            "Epoch 47/100\n",
            "323/323 [==============================] - 0s 331us/step - loss: 0.0092 - val_loss: 0.0093\n",
            "Epoch 48/100\n",
            "323/323 [==============================] - 0s 319us/step - loss: 0.0095 - val_loss: 0.0092\n",
            "Epoch 49/100\n",
            "323/323 [==============================] - 0s 307us/step - loss: 0.0092 - val_loss: 0.0094\n",
            "Epoch 50/100\n",
            "323/323 [==============================] - 0s 323us/step - loss: 0.0091 - val_loss: 0.0093\n",
            "Epoch 51/100\n",
            "323/323 [==============================] - 0s 320us/step - loss: 0.0091 - val_loss: 0.0094\n",
            "Epoch 52/100\n",
            "323/323 [==============================] - 0s 313us/step - loss: 0.0090 - val_loss: 0.0094\n",
            "Epoch 53/100\n",
            "323/323 [==============================] - 0s 297us/step - loss: 0.0091 - val_loss: 0.0093\n",
            "Epoch 54/100\n",
            "323/323 [==============================] - 0s 336us/step - loss: 0.0101 - val_loss: 0.0094\n",
            "Epoch 55/100\n",
            "323/323 [==============================] - 0s 303us/step - loss: 0.0092 - val_loss: 0.0093\n",
            "Epoch 56/100\n",
            "323/323 [==============================] - 0s 313us/step - loss: 0.0091 - val_loss: 0.0094\n",
            "Epoch 57/100\n",
            "323/323 [==============================] - 0s 320us/step - loss: 0.0090 - val_loss: 0.0095\n",
            "Epoch 58/100\n",
            "323/323 [==============================] - 0s 314us/step - loss: 0.0093 - val_loss: 0.0094\n",
            "Epoch 59/100\n",
            "323/323 [==============================] - 0s 322us/step - loss: 0.0090 - val_loss: 0.0094\n",
            "Epoch 60/100\n",
            "323/323 [==============================] - 0s 292us/step - loss: 0.0090 - val_loss: 0.0094\n",
            "Epoch 61/100\n",
            "323/323 [==============================] - 0s 284us/step - loss: 0.0089 - val_loss: 0.0093\n",
            "Epoch 62/100\n",
            "323/323 [==============================] - 0s 299us/step - loss: 0.0087 - val_loss: 0.0092\n",
            "Epoch 63/100\n",
            "323/323 [==============================] - 0s 288us/step - loss: 0.0086 - val_loss: 0.0092\n",
            "Epoch 64/100\n",
            "323/323 [==============================] - 0s 321us/step - loss: 0.0087 - val_loss: 0.0092\n",
            "Epoch 65/100\n",
            "323/323 [==============================] - 0s 319us/step - loss: 0.0086 - val_loss: 0.0091\n",
            "Epoch 66/100\n",
            "323/323 [==============================] - 0s 322us/step - loss: 0.0086 - val_loss: 0.0092\n",
            "Epoch 67/100\n",
            "323/323 [==============================] - 0s 342us/step - loss: 0.0085 - val_loss: 0.0092\n",
            "Epoch 68/100\n",
            "323/323 [==============================] - 0s 333us/step - loss: 0.0085 - val_loss: 0.0091\n",
            "Epoch 69/100\n",
            "323/323 [==============================] - 0s 309us/step - loss: 0.0085 - val_loss: 0.0092\n",
            "Epoch 70/100\n",
            "323/323 [==============================] - 0s 287us/step - loss: 0.0085 - val_loss: 0.0092\n",
            "Epoch 71/100\n",
            "323/323 [==============================] - 0s 328us/step - loss: 0.0084 - val_loss: 0.0091\n",
            "Epoch 72/100\n",
            "323/323 [==============================] - 0s 304us/step - loss: 0.0085 - val_loss: 0.0093\n",
            "Epoch 73/100\n",
            "323/323 [==============================] - 0s 377us/step - loss: 0.0085 - val_loss: 0.0091\n",
            "Epoch 74/100\n",
            "323/323 [==============================] - 0s 347us/step - loss: 0.0083 - val_loss: 0.0090\n",
            "Epoch 75/100\n",
            "323/323 [==============================] - 0s 340us/step - loss: 0.0084 - val_loss: 0.0090\n",
            "Epoch 76/100\n",
            "323/323 [==============================] - 0s 333us/step - loss: 0.0085 - val_loss: 0.0091\n",
            "Epoch 77/100\n",
            "323/323 [==============================] - 0s 317us/step - loss: 0.0086 - val_loss: 0.0091\n",
            "Epoch 78/100\n",
            "323/323 [==============================] - 0s 346us/step - loss: 0.0083 - val_loss: 0.0090\n",
            "Epoch 79/100\n",
            "323/323 [==============================] - 0s 320us/step - loss: 0.0081 - val_loss: 0.0091\n",
            "Epoch 80/100\n",
            "323/323 [==============================] - 0s 306us/step - loss: 0.0081 - val_loss: 0.0091\n",
            "Epoch 81/100\n",
            "323/323 [==============================] - 0s 309us/step - loss: 0.0082 - val_loss: 0.0092\n",
            "Epoch 82/100\n",
            "323/323 [==============================] - 0s 348us/step - loss: 0.0086 - val_loss: 0.0093\n",
            "Epoch 83/100\n",
            "323/323 [==============================] - 0s 326us/step - loss: 0.0085 - val_loss: 0.0092\n",
            "Epoch 84/100\n",
            "323/323 [==============================] - 0s 318us/step - loss: 0.0081 - val_loss: 0.0091\n",
            "Epoch 85/100\n",
            "323/323 [==============================] - 0s 330us/step - loss: 0.0080 - val_loss: 0.0091\n",
            "Epoch 86/100\n",
            "323/323 [==============================] - 0s 338us/step - loss: 0.0080 - val_loss: 0.0091\n",
            "Epoch 87/100\n",
            "323/323 [==============================] - 0s 311us/step - loss: 0.0080 - val_loss: 0.0091\n",
            "Epoch 88/100\n",
            "323/323 [==============================] - 0s 300us/step - loss: 0.0078 - val_loss: 0.0091\n",
            "Epoch 89/100\n",
            "323/323 [==============================] - 0s 305us/step - loss: 0.0079 - val_loss: 0.0091\n",
            "Epoch 90/100\n",
            "323/323 [==============================] - 0s 320us/step - loss: 0.0079 - val_loss: 0.0091\n",
            "Epoch 91/100\n",
            "323/323 [==============================] - 0s 327us/step - loss: 0.0081 - val_loss: 0.0090\n",
            "Epoch 92/100\n",
            "323/323 [==============================] - 0s 288us/step - loss: 0.0082 - val_loss: 0.0090\n",
            "Epoch 93/100\n",
            "323/323 [==============================] - 0s 327us/step - loss: 0.0079 - val_loss: 0.0090\n",
            "Epoch 94/100\n",
            "323/323 [==============================] - 0s 294us/step - loss: 0.0077 - val_loss: 0.0091\n",
            "Epoch 95/100\n",
            "323/323 [==============================] - 0s 312us/step - loss: 0.0077 - val_loss: 0.0090\n",
            "Epoch 96/100\n",
            "323/323 [==============================] - 0s 308us/step - loss: 0.0079 - val_loss: 0.0088\n",
            "Epoch 97/100\n",
            "323/323 [==============================] - 0s 293us/step - loss: 0.0079 - val_loss: 0.0089\n",
            "Epoch 98/100\n",
            "323/323 [==============================] - 0s 311us/step - loss: 0.0078 - val_loss: 0.0088\n",
            "Epoch 99/100\n",
            "323/323 [==============================] - 0s 317us/step - loss: 0.0077 - val_loss: 0.0088\n",
            "Epoch 100/100\n",
            "323/323 [==============================] - 0s 346us/step - loss: 0.0078 - val_loss: 0.0090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mia7bwVPeyuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "pca = PCA(n_components=11)\n",
        "x_train_pca = pca.fit_transform(x_train)\n",
        "x_test_pca  = pca.transform(x_test)\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "np.sum(explained_var[0:11])\n",
        "pca_out = explained_var[0:11]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br9NrgL4ezOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#128,16,1\n",
        "model_pca = Sequential([\n",
        "    Dense(10, activation='relu', input_shape=(11,)),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dense(1, kernel_initializer='normal')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kPkaPbCe5FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_pca.compile(loss='mean_squared_error', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOYck8zde9w4",
        "colab_type": "code",
        "outputId": "95dc31eb-d2d6-411e-b0b8-7f58c4e84a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trained_model_pca = model_pca.fit(x_train_pca, y_train, batch_size=16, epochs=100, validation_split=0.2)\n",
        "history_pca = trained_model_pca.history"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 323 samples, validate on 81 samples\n",
            "Epoch 1/100\n",
            "323/323 [==============================] - 1s 4ms/step - loss: 0.1985 - val_loss: 0.1975\n",
            "Epoch 2/100\n",
            "323/323 [==============================] - 0s 353us/step - loss: 0.1721 - val_loss: 0.1714\n",
            "Epoch 3/100\n",
            "323/323 [==============================] - 0s 335us/step - loss: 0.1477 - val_loss: 0.1437\n",
            "Epoch 4/100\n",
            "323/323 [==============================] - 0s 317us/step - loss: 0.1203 - val_loss: 0.1118\n",
            "Epoch 5/100\n",
            "323/323 [==============================] - 0s 318us/step - loss: 0.0899 - val_loss: 0.0772\n",
            "Epoch 6/100\n",
            "323/323 [==============================] - 0s 308us/step - loss: 0.0590 - val_loss: 0.0467\n",
            "Epoch 7/100\n",
            "323/323 [==============================] - 0s 344us/step - loss: 0.0362 - val_loss: 0.0303\n",
            "Epoch 8/100\n",
            "323/323 [==============================] - 0s 319us/step - loss: 0.0268 - val_loss: 0.0243\n",
            "Epoch 9/100\n",
            "323/323 [==============================] - 0s 317us/step - loss: 0.0234 - val_loss: 0.0219\n",
            "Epoch 10/100\n",
            "323/323 [==============================] - 0s 293us/step - loss: 0.0216 - val_loss: 0.0202\n",
            "Epoch 11/100\n",
            "323/323 [==============================] - 0s 290us/step - loss: 0.0202 - val_loss: 0.0190\n",
            "Epoch 12/100\n",
            "323/323 [==============================] - 0s 311us/step - loss: 0.0192 - val_loss: 0.0181\n",
            "Epoch 13/100\n",
            "323/323 [==============================] - 0s 321us/step - loss: 0.0183 - val_loss: 0.0171\n",
            "Epoch 14/100\n",
            "323/323 [==============================] - 0s 306us/step - loss: 0.0175 - val_loss: 0.0163\n",
            "Epoch 15/100\n",
            "323/323 [==============================] - 0s 327us/step - loss: 0.0167 - val_loss: 0.0155\n",
            "Epoch 16/100\n",
            "323/323 [==============================] - 0s 306us/step - loss: 0.0160 - val_loss: 0.0147\n",
            "Epoch 17/100\n",
            "323/323 [==============================] - 0s 334us/step - loss: 0.0153 - val_loss: 0.0139\n",
            "Epoch 18/100\n",
            "323/323 [==============================] - 0s 298us/step - loss: 0.0146 - val_loss: 0.0131\n",
            "Epoch 19/100\n",
            "323/323 [==============================] - 0s 330us/step - loss: 0.0138 - val_loss: 0.0124\n",
            "Epoch 20/100\n",
            "323/323 [==============================] - 0s 332us/step - loss: 0.0133 - val_loss: 0.0118\n",
            "Epoch 21/100\n",
            "323/323 [==============================] - 0s 303us/step - loss: 0.0127 - val_loss: 0.0113\n",
            "Epoch 22/100\n",
            "323/323 [==============================] - 0s 309us/step - loss: 0.0122 - val_loss: 0.0108\n",
            "Epoch 23/100\n",
            "323/323 [==============================] - 0s 325us/step - loss: 0.0116 - val_loss: 0.0104\n",
            "Epoch 24/100\n",
            "323/323 [==============================] - 0s 278us/step - loss: 0.0113 - val_loss: 0.0101\n",
            "Epoch 25/100\n",
            "323/323 [==============================] - 0s 302us/step - loss: 0.0110 - val_loss: 0.0099\n",
            "Epoch 26/100\n",
            "323/323 [==============================] - 0s 312us/step - loss: 0.0105 - val_loss: 0.0096\n",
            "Epoch 27/100\n",
            "323/323 [==============================] - 0s 373us/step - loss: 0.0104 - val_loss: 0.0097\n",
            "Epoch 28/100\n",
            "323/323 [==============================] - 0s 308us/step - loss: 0.0099 - val_loss: 0.0093\n",
            "Epoch 29/100\n",
            "323/323 [==============================] - 0s 304us/step - loss: 0.0096 - val_loss: 0.0091\n",
            "Epoch 30/100\n",
            "323/323 [==============================] - 0s 308us/step - loss: 0.0094 - val_loss: 0.0091\n",
            "Epoch 31/100\n",
            "323/323 [==============================] - 0s 312us/step - loss: 0.0092 - val_loss: 0.0089\n",
            "Epoch 32/100\n",
            "323/323 [==============================] - 0s 307us/step - loss: 0.0091 - val_loss: 0.0088\n",
            "Epoch 33/100\n",
            "323/323 [==============================] - 0s 341us/step - loss: 0.0089 - val_loss: 0.0087\n",
            "Epoch 34/100\n",
            "323/323 [==============================] - 0s 301us/step - loss: 0.0087 - val_loss: 0.0086\n",
            "Epoch 35/100\n",
            "323/323 [==============================] - 0s 300us/step - loss: 0.0085 - val_loss: 0.0086\n",
            "Epoch 36/100\n",
            "323/323 [==============================] - 0s 311us/step - loss: 0.0084 - val_loss: 0.0086\n",
            "Epoch 37/100\n",
            "323/323 [==============================] - 0s 316us/step - loss: 0.0083 - val_loss: 0.0085\n",
            "Epoch 38/100\n",
            "323/323 [==============================] - 0s 323us/step - loss: 0.0082 - val_loss: 0.0084\n",
            "Epoch 39/100\n",
            "323/323 [==============================] - 0s 329us/step - loss: 0.0081 - val_loss: 0.0084\n",
            "Epoch 40/100\n",
            "323/323 [==============================] - 0s 315us/step - loss: 0.0079 - val_loss: 0.0082\n",
            "Epoch 41/100\n",
            "323/323 [==============================] - 0s 315us/step - loss: 0.0078 - val_loss: 0.0082\n",
            "Epoch 42/100\n",
            "323/323 [==============================] - 0s 284us/step - loss: 0.0077 - val_loss: 0.0082\n",
            "Epoch 43/100\n",
            "323/323 [==============================] - 0s 298us/step - loss: 0.0076 - val_loss: 0.0082\n",
            "Epoch 44/100\n",
            "323/323 [==============================] - 0s 288us/step - loss: 0.0075 - val_loss: 0.0081\n",
            "Epoch 45/100\n",
            "323/323 [==============================] - 0s 294us/step - loss: 0.0074 - val_loss: 0.0080\n",
            "Epoch 46/100\n",
            "323/323 [==============================] - 0s 303us/step - loss: 0.0073 - val_loss: 0.0079\n",
            "Epoch 47/100\n",
            "323/323 [==============================] - 0s 349us/step - loss: 0.0072 - val_loss: 0.0078\n",
            "Epoch 48/100\n",
            "323/323 [==============================] - 0s 309us/step - loss: 0.0071 - val_loss: 0.0078\n",
            "Epoch 49/100\n",
            "323/323 [==============================] - 0s 314us/step - loss: 0.0070 - val_loss: 0.0078\n",
            "Epoch 50/100\n",
            "323/323 [==============================] - 0s 309us/step - loss: 0.0069 - val_loss: 0.0077\n",
            "Epoch 51/100\n",
            "323/323 [==============================] - 0s 314us/step - loss: 0.0069 - val_loss: 0.0076\n",
            "Epoch 52/100\n",
            "323/323 [==============================] - 0s 293us/step - loss: 0.0068 - val_loss: 0.0075\n",
            "Epoch 53/100\n",
            "323/323 [==============================] - 0s 307us/step - loss: 0.0067 - val_loss: 0.0074\n",
            "Epoch 54/100\n",
            "323/323 [==============================] - 0s 348us/step - loss: 0.0065 - val_loss: 0.0073\n",
            "Epoch 55/100\n",
            "323/323 [==============================] - 0s 312us/step - loss: 0.0065 - val_loss: 0.0073\n",
            "Epoch 56/100\n",
            "323/323 [==============================] - 0s 330us/step - loss: 0.0064 - val_loss: 0.0073\n",
            "Epoch 57/100\n",
            "323/323 [==============================] - 0s 343us/step - loss: 0.0063 - val_loss: 0.0072\n",
            "Epoch 58/100\n",
            "323/323 [==============================] - 0s 322us/step - loss: 0.0063 - val_loss: 0.0071\n",
            "Epoch 59/100\n",
            "323/323 [==============================] - 0s 317us/step - loss: 0.0062 - val_loss: 0.0071\n",
            "Epoch 60/100\n",
            "323/323 [==============================] - 0s 293us/step - loss: 0.0061 - val_loss: 0.0070\n",
            "Epoch 61/100\n",
            "323/323 [==============================] - 0s 321us/step - loss: 0.0061 - val_loss: 0.0069\n",
            "Epoch 62/100\n",
            "323/323 [==============================] - 0s 302us/step - loss: 0.0060 - val_loss: 0.0069\n",
            "Epoch 63/100\n",
            "323/323 [==============================] - 0s 357us/step - loss: 0.0059 - val_loss: 0.0069\n",
            "Epoch 64/100\n",
            "323/323 [==============================] - 0s 298us/step - loss: 0.0058 - val_loss: 0.0068\n",
            "Epoch 65/100\n",
            "323/323 [==============================] - 0s 325us/step - loss: 0.0058 - val_loss: 0.0069\n",
            "Epoch 66/100\n",
            "323/323 [==============================] - 0s 334us/step - loss: 0.0057 - val_loss: 0.0067\n",
            "Epoch 67/100\n",
            "323/323 [==============================] - 0s 317us/step - loss: 0.0057 - val_loss: 0.0066\n",
            "Epoch 68/100\n",
            "323/323 [==============================] - 0s 324us/step - loss: 0.0056 - val_loss: 0.0066\n",
            "Epoch 69/100\n",
            "323/323 [==============================] - 0s 326us/step - loss: 0.0056 - val_loss: 0.0065\n",
            "Epoch 70/100\n",
            "323/323 [==============================] - 0s 325us/step - loss: 0.0055 - val_loss: 0.0065\n",
            "Epoch 71/100\n",
            "323/323 [==============================] - 0s 295us/step - loss: 0.0055 - val_loss: 0.0064\n",
            "Epoch 72/100\n",
            "323/323 [==============================] - 0s 301us/step - loss: 0.0054 - val_loss: 0.0065\n",
            "Epoch 73/100\n",
            "323/323 [==============================] - 0s 359us/step - loss: 0.0053 - val_loss: 0.0068\n",
            "Epoch 74/100\n",
            "323/323 [==============================] - 0s 318us/step - loss: 0.0054 - val_loss: 0.0065\n",
            "Epoch 75/100\n",
            "323/323 [==============================] - 0s 313us/step - loss: 0.0053 - val_loss: 0.0065\n",
            "Epoch 76/100\n",
            "323/323 [==============================] - 0s 333us/step - loss: 0.0052 - val_loss: 0.0063\n",
            "Epoch 77/100\n",
            "323/323 [==============================] - 0s 317us/step - loss: 0.0052 - val_loss: 0.0064\n",
            "Epoch 78/100\n",
            "323/323 [==============================] - 0s 334us/step - loss: 0.0051 - val_loss: 0.0062\n",
            "Epoch 79/100\n",
            "323/323 [==============================] - 0s 308us/step - loss: 0.0051 - val_loss: 0.0063\n",
            "Epoch 80/100\n",
            "323/323 [==============================] - 0s 307us/step - loss: 0.0050 - val_loss: 0.0063\n",
            "Epoch 81/100\n",
            "323/323 [==============================] - 0s 308us/step - loss: 0.0050 - val_loss: 0.0063\n",
            "Epoch 82/100\n",
            "323/323 [==============================] - 0s 295us/step - loss: 0.0050 - val_loss: 0.0062\n",
            "Epoch 83/100\n",
            "323/323 [==============================] - 0s 339us/step - loss: 0.0049 - val_loss: 0.0060\n",
            "Epoch 84/100\n",
            "323/323 [==============================] - 0s 304us/step - loss: 0.0048 - val_loss: 0.0062\n",
            "Epoch 85/100\n",
            "323/323 [==============================] - 0s 336us/step - loss: 0.0048 - val_loss: 0.0063\n",
            "Epoch 86/100\n",
            "323/323 [==============================] - 0s 355us/step - loss: 0.0047 - val_loss: 0.0062\n",
            "Epoch 87/100\n",
            "323/323 [==============================] - 0s 327us/step - loss: 0.0047 - val_loss: 0.0061\n",
            "Epoch 88/100\n",
            "323/323 [==============================] - 0s 312us/step - loss: 0.0046 - val_loss: 0.0061\n",
            "Epoch 89/100\n",
            "323/323 [==============================] - 0s 323us/step - loss: 0.0046 - val_loss: 0.0062\n",
            "Epoch 90/100\n",
            "323/323 [==============================] - 0s 331us/step - loss: 0.0046 - val_loss: 0.0061\n",
            "Epoch 91/100\n",
            "323/323 [==============================] - 0s 312us/step - loss: 0.0045 - val_loss: 0.0062\n",
            "Epoch 92/100\n",
            "323/323 [==============================] - 0s 328us/step - loss: 0.0045 - val_loss: 0.0063\n",
            "Epoch 93/100\n",
            "323/323 [==============================] - 0s 326us/step - loss: 0.0045 - val_loss: 0.0062\n",
            "Epoch 94/100\n",
            "323/323 [==============================] - 0s 325us/step - loss: 0.0046 - val_loss: 0.0064\n",
            "Epoch 95/100\n",
            "323/323 [==============================] - 0s 320us/step - loss: 0.0044 - val_loss: 0.0062\n",
            "Epoch 96/100\n",
            "323/323 [==============================] - 0s 305us/step - loss: 0.0043 - val_loss: 0.0062\n",
            "Epoch 97/100\n",
            "323/323 [==============================] - 0s 300us/step - loss: 0.0043 - val_loss: 0.0063\n",
            "Epoch 98/100\n",
            "323/323 [==============================] - 0s 316us/step - loss: 0.0042 - val_loss: 0.0064\n",
            "Epoch 99/100\n",
            "323/323 [==============================] - 0s 315us/step - loss: 0.0042 - val_loss: 0.0064\n",
            "Epoch 100/100\n",
            "323/323 [==============================] - 0s 335us/step - loss: 0.0042 - val_loss: 0.0062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojj5gTczlQoh",
        "colab_type": "code",
        "outputId": "4f5d65ee-f0ed-4189-9d68-f0e3578eee88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "max_loss = max(history['val_loss'])\n",
        "min_loss = min(history['val_loss'])\n",
        "\n",
        "max_loss_pca = max(history_pca['val_loss'])\n",
        "min_loss_pca = min(history_pca['val_loss'])\n",
        "\n",
        "print('Max_loss and Min_loss are', max_loss,',', min_loss,'.')\n",
        "print('Max_loss_pca and Min_loss_pca are', max_loss_pca,',', min_loss_pca,'.')"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max_loss and Min_loss are 0.16885385965859448 , 0.008811200713094922 .\n",
            "Max_loss_pca and Min_loss_pca are 0.19748268607589933 , 0.006025095210471018 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgAZJN6elY3X",
        "colab_type": "code",
        "outputId": "19e87c54-6217-49b5-858a-e2015447ceaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Loss_ = history['val_loss']\n",
        "Loss_pca = history_pca['val_loss']\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('val_Loss')\n",
        "plt.plot(Loss_, 'darkred')\n",
        "plt.plot(Loss_pca,'darkgreen')\n",
        "plt.legend(['val_loss without pca','val_loss with pca'])\n",
        "plt.show()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEGCAYAAACdJRn3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npsf+H3ziIHb4yE86AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}